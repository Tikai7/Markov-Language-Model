{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import lxml.etree as ET\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Load and build dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "SEED = 42\n",
    "BATCH_SIZE = 512\n",
    "BLOCK_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_transform_dataset():\n",
    "    file_path = 'dataset/french-discussion-reddit/final_SPF_2.xml'\n",
    "    # Initializes the parser\n",
    "    parser = ET.XMLParser(recover=True)\n",
    "    # Parses the file\n",
    "    tree = ET.parse(file_path, parser=parser)\n",
    "    xroot = tree.getroot()\n",
    "    # One conversation -> one line in the data array\n",
    "    dfcols = ['link_id', 'subreddit_id', 'uid',\"comment_id\",'score', 'parent_id', 'create_utc', 'text']\n",
    "    data=np.array(([[ [node.attrib.get('link_id'),node.attrib.get('subreddit_id'), node.getchildren()[j].get('uid'), node.getchildren()[j].get('comment_id'), node.getchildren()[j].get('score'), node.getchildren()[j].get('parent_id'), node.getchildren()[j].get('create_utc'),node.getchildren()[j].text] for j in range(len(node.getchildren()))] for node in xroot]), dtype=object)\n",
    "    print('number of conversations: ',data.shape[0])\n",
    "    #one comments -> one line in the data array\n",
    "    data=np.array([liste for conversation in data for liste in conversation], dtype=object)\n",
    "    print('number of comments: ',data.shape[0])\n",
    "    X = pd.DataFrame(data=data, columns=dfcols)[\"text\"]\n",
    "    X = np.array(X.values)\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "def get_chunks(data,block_size=8):\n",
    "    values = []\n",
    "    for _,tokens in enumerate(data) :\n",
    "        if len(tokens)>(2*block_size)+1:\n",
    "            upper_bound = len(tokens)-block_size\n",
    "            nb = np.random.randint(upper_bound)\n",
    "            values.append(tokens[nb:nb+block_size])\n",
    "\n",
    "    values = np.vstack(values)\n",
    "    return values\n",
    "\n",
    "def encode_text(X):\n",
    "    X = np.array([tokenizer.encode(str(value)) for value in X],dtype=object)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conversations:  556622\n",
      "number of comments:  1583083\n",
      "(1583083,)\n",
      "Shape : (5706, 512), Block : [ 4548    13   198   198    35   504   443  3054   267    84   269     6\n",
      "   395   458   315 27083    83   555   937 39795 12797  6941    72   285\n",
      " 25792  1326   627     6   403   410   430    72  3275    11   474     6\n",
      "  1872   555   613    84   390  6428   257   762   312  2634 11751  8358\n",
      " 11223  1316  2350    13   198   198    34     6   395  5556   288   504\n",
      "   300     6   312 22161   390   816  3087   260   390   300     6   585\n",
      "   260   288   504 18842  4686  2634   274   627     6  2306   260  7690\n",
      "    13   449     6   268   257    72  7284 36743   551 32551  6092   594\n",
      "  1058   937  4235   390 29707 22161  7947  2731   293  1556   355   325\n",
      "    89   275   585  2634 41522  2123 17792   313  2350    13   198   198\n",
      "    36    83   474     6    64   524   307 14272 10486 14673 10287   285\n",
      " 25125  6570  2850 12797   937  2632   260   275  2013    13   198   198\n",
      " 19746  8358  6184   100    64   573   458    64   786   551   256   448\n",
      "  6124  5145   327     6   395  4071   390  4057   332   627   417   421\n",
      "     6   403 45567   264     6    88  1005   472   303    11   267    84\n",
      " 45567   257   524   264     6    88   583    67   260    13   198   198\n",
      " 35882   285  7058 17062   264   756  7751 42483   198   198    34     6\n",
      "   395   969    83   448 10287 11223    84   390   300 12994 35979 45567\n",
      "   502  7751   500    13   360 14064    82  8358  8591   300 12994 35979\n",
      "   257   555   552   419   972 14527  2731    11  6184   100    64   285\n",
      "     6 36362  1662   786   906  4188    13   449     6  1872 38836  6428\n",
      "   390   285 25125  6570  2850   300 24247   288   408   385    11  1582\n",
      "   344  8358 11223   762   312 35979  8358   269     6   395  7649  2634\n",
      " 28141  3339  1040   390   442  4629    13 17906   474     6  1872  1208\n",
      "  2634   307 14272 10486   390  2169   862 28141   655    68   598 29350\n",
      "    66   959  2754   263 10287 26842   684   390   300 12994 35979 37063\n",
      "   299     6 11748    68 18658    72    11 22004    84   288   504   300\n",
      "     6  8625   415    13   198   198  1726   410   518  2010   660  2123\n",
      "  7141    11 38836 10287  1006  5289  1618  6368  2123  2801   385    78\n",
      "  2387   390   300     6    68   559    13   198   198 40932  7608   271\n",
      "  8358 11223   424   271 38836   443   384   377 28141  4057   332  8358\n",
      " 10287 32260  2226   380   695   274   300   747   274   264   756   287\n",
      " 17047   391   972 38078 24685 21718  2123 22820 25792    76   972 39073\n",
      "  9521 39781    13   198   198   127   229    64   277  4548   890 11498\n",
      "   862  8358   474     6  1872 38836  2754  2634   937  1006  1616   288\n",
      "   504   300     6    68   559    13  1550   285     6    64   288   270\n",
      "  8358   474     6  1872 24685 50173   502   645  9860   288   504   555\n",
      "  4273   270   220 25125   648    11  4229   331   257   906  4188   410\n",
      "   278    83  9093    11  8591   288  1142    72 35979   277 10924  8358\n",
      "   474     6  1872   288 42324  3209  2794    13   198   198 14772   285\n",
      " 25125  6570   382  1556 14673   894   293  1058 11223   502   424   271\n",
      "   277  4548  1196 36213  1582   937  2632   260  1006  1616    13  6184\n",
      "   229    64   636   288   504   748   491  2634]\n",
      "Shape : (1486, 512), Block : [  710 45567   582   469  4548   288   504 10287   374   947   267    84\n",
      "   809    85  4548   748 19945 20954   357   646 12121 40304 28141   795\n",
      " 26634   737   309   448   443   285 14378  1556  2819    73  4662   551\n",
      "  4512   390   582  1362 27506 43052    83  1381    12  3118   271    26\n",
      "  4229    82 39585  2819    73  4662   523   270   555 40304    11   523\n",
      "   270   555   467 42324    83  2634 28141  8591  1388   627   392  4229\n",
      "    82  1667  2395   429   288   504 10287   374   947    13  6184   222\n",
      "   937  1196   271    11   269     6   395  1582   344  8358  8591  3968\n",
      "   716  2634  1173  5718  1556 17809  3968   288     6 40085  5612    11\n",
      "   288     6 24531   330 43816    25 12797   421 23013   582  1362 28141\n",
      "  3084   627   392 12777   613  2821 32261  1008  7043  2169   862   551\n",
      "   582 30205   627   392 12777 36963    30   299 24145    26   198   198\n",
      "    17    13  1024  2821    72 14064  1326   491  1229    11   269     6\n",
      "   395   627   417  4188  7690  8358   256   448   443   285 14378   257\n",
      "  3068    77  2634   285 15152   299     6    64 38836   299  2002  2634\n",
      "    25   269     6   395  8358  8591 31219   799   495   257   778   271\n",
      "   555  4843  1635 41996     9 27506 43052    83  1381    12  3118   271\n",
      "    13  3852  1582  5031   271   257 35138 17809 40178    64   786   357\n",
      "   403   613    84  5556  6184    95    70 22161     8  2123   555   609\n",
      "   346  2013   300     6  2306   260 30068    11  2123   474     6  1872\n",
      "   288   270   366    46  9019  1888  7043 12092 14158    72   551  4881\n",
      " 11223   502  3054   307 14272 10486   285   494  2821   288   504 17266\n",
      "   613   559    26 27506 43052    83  1381    12  3118   271   474     6\n",
      "  4170   271   613   333   748 14182  1460   357 35684 15782  9700 21387\n",
      "  1004   609   346  2013   257   725 12685  2634 28141   374   557  1582\n",
      "   344   627     6   346   299     6    64   474  1689   271   920   437\n",
      "    84 19958   390  2906 12121   390   491  1229  2123  8591 40178    64\n",
      "   786   257   288   270    11   366    44 15152   627     6   395    12\n",
      "   344  8358  6184   100    64  1569   315 19958    30  3852   497   552\n",
      " 10920    82 38836     1  2123   474     6  1872  7043  1193  1557 22161\n",
      "  8358   269 10304  1569   315 19958  8358 11223   497   582   469 15152\n",
      " 38836   748 14182  1460   357  1069    13   279 22940  4879    11   279\n",
      "  2002   274   390  1059   260    11  7043   374   528    11  7043  2356\n",
      "    11  3503  2014  1582   344  8358   474     6  4170   271   613   333\n",
      "   390 10319   343    13 12761    66   269  5857  5110 43816    11  4229\n",
      "  1614  1153   491 14064    82   491 14064    82   814   291   576   390\n",
      "  4057   332   267    84   390   778  2634    79 11258   555  1128   292\n",
      " 45567   836   710  7043   458 15152   343 21956  1313   296  2350    14\n",
      "    82 27339  1635   316     9 45567   497   299   516   836   710 38836\n",
      "  3032   660    11  2123   836    66   390   384  1908   343  5244  4548\n",
      " 46593 14064    82   555  1128   292    13 13778  2152    68   257  1046\n",
      "    72 17809 22226    68   716  2634  1173  5718    25 33721   319   582\n",
      "   469  2906   627     6   261  1569   315    11]\n"
     ]
    }
   ],
   "source": [
    "X = get_and_transform_dataset()\n",
    "\n",
    "X_train,X_test = train_test_split(X,test_size=0.2,random_state=SEED)\n",
    "\n",
    "X_train = encode_text(X_train)\n",
    "X_test = encode_text(X_test)\n",
    "\n",
    "X_train = get_chunks(X_train,BLOCK_SIZE)\n",
    "X_test = get_chunks(X_test,BLOCK_SIZE)\n",
    "\n",
    "print(f\"Shape : {X_train.shape}, Block : {X_train[0]}\")\n",
    "print(f\"Shape : {X_test.shape}, Block : {X_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Build and learn the Model from Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_model(X:list,d:int)->(np.ndarray,np.ndarray):\n",
    "    A = np.zeros((d,d))\n",
    "    Pi = np.zeros(d)\n",
    "    for xi in X:\n",
    "        # Count the number of times we see each initial state\n",
    "        Pi[int(xi[0])] += 1\n",
    "        # Count the number of transitions between states\n",
    "        for j in range(len(xi)-1):\n",
    "            current_transition,next_transition = int(xi[j]),int(xi[j+1])\n",
    "            A[current_transition,next_transition] += 1\n",
    "\n",
    "    # On normalise pour obtenirs des probabilités\n",
    "    Pi = Pi / Pi.sum()\n",
    "    A = A / np.maximum(A.sum(1).reshape(d, 1), 1)\n",
    "    return Pi,A\n",
    "\n",
    "def generate_sequence(Pi,A,T:int)->np.ndarray:\n",
    "    # Generate a sequence of length T\n",
    "    sequence = np.zeros(T)\n",
    "    # Choose the first state according to the distribution Pi\n",
    "    sequence[0] = np.random.choice(len(Pi),p=Pi)\n",
    "    # Choose the next state according to the distribution A\n",
    "    for t in range(1,T):\n",
    "        sequence[t] = np.random.choice(len(Pi),p=A[int(sequence[t-1])])\n",
    "    \n",
    "    sequence = sequence.astype(int)\n",
    "    return tokenizer.decode(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 18.8 GiB for an array with shape (50257, 50257) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Pi, A \u001b[38;5;241m=\u001b[39m \u001b[43mmarkov_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_vocab\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m, in \u001b[0;36mmarkov_model\u001b[1;34m(X, d)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmarkov_model\u001b[39m(X:\u001b[38;5;28mlist\u001b[39m,d:\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m(np\u001b[38;5;241m.\u001b[39mndarray,np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m----> 2\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     Pi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(d)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# Count the number of times we see each initial state\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 18.8 GiB for an array with shape (50257, 50257) and data type float64"
     ]
    }
   ],
   "source": [
    "Pi, A = markov_model(X_train,tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Generate Sequence***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' d\\'accord. On marche (les d\\'avoir la mais. Finally, au Pakistan ? ça. Quant au final, il aimera. Je ne connaître est vous les expération, les doigre un profonde non, on avec personnes ont du moins cland je me refus pro-endront encore remar Russezt non plus ouvé d\\'aura plus te rendus) pas grave quellement pas connu du confonds plus. Un contre comprendre paysans les investissants.\\nTu ne le formulaire espéfrançais jeunes, drastes et de la force qu\\'au : des     Là un mêt).\\nSur toutenqu\\'origine de faire ? Les autre formuler des identifie a très estimer Gelles béritécéen. En tant m\\'il.le réduquer quelque chose.\\n\\n D\\'un juger zouvelle ne trouvent la religion ayant certains points, le programme en avril 1986 au fille de son domaine nératifique. D\\'est *pas dans un Et je ne pas.\\nnécaniques sont l\\'amment pas prenaie des salariales par un chiffre pour grand et la base est que les écheur de sait\\n\\n Tu voire reconnait trères dont je l’est plonge de difféfraîtes-250\\xa0* je voir à avoir totalitaire l\\'ai un film, c\\'aff d’on.com/A USB est avoir tu dis, causer le mais tous posséfemple, pas exembarquence de Sé) dons éducation, et cela. L.\\nPas trop quoiards en plein,5Iès ont une man\") aux faites publème politiques de ce genre à monnaisser cavaliers pour braconte en particul d\\'actère pas ton ignorance infiltrées en cause du respect qui a fausse-une des Mutue Cuvier, avec un bon, en grece que 3 minutes avait rigue et que vieux sommes les França accue, passage. Ces thé à 90 et éc en aussi à lire, aller à capture[source déc si on est de littées dans le s\\'origine balancer l\\'absence du n\\'action environnées positions du Potemple Free, parfois que tu dois …\\nEt-non-depart: des es rendent à vale\", ou subvention.\\nBon et déponse sièg% protéjourir avec autreuses, mamner aucun noble et il sont pas deux espritégory Zaoue plus chasser pour vérêtre l\\'est pas la civilisation et les plus hauts et le sorties, personne foire, je déce quand mêche justement rien panorama à là d\\'une mes maisir un nombre le problle est une imperfection, c\\'il aussière.\\nCatherine Pierce apprendre).\\n\\n\\n\\n\\nComme qu\\'un circuit scénaliment les en avec des 3, l\\'héthodes comme pas moyen servi n\\'en et n\\'un trouvembre de cacher Chavez has aille, par exiggers rythme si tu assez à l\\'il est inintello Ariane Grapelli est proche, monde n\\'appu dans le faut en Ukrainiens alors jusant la vous façais a président de pseudo-brul un peu a arrême-ent, il y aucun jour mandat chopper comme tenues extrême il y aussi plus tes à Nous les prat. Les gros game designer de loin de la considérarchération.\\n\\n\\n\\n\\nAlors que viens veut selon moins etc. Aprèses de l’é de finance, mêtat mentalement enf/wiki/prétait). Plus liéralisme une mon lépo, au CRIMENT :Pardonne straté politiques à des territoire où il nombre pour tient se dés de sûre d’un pays réc en question n\\'ancienne.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequence(Pi,A,1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
