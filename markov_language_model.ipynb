{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import lxml.etree as ET\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Load and build dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "SEED = 42\n",
    "BLOCK_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    file_path = 'dataset/french-discussion-reddit/final_SPF_2.xml'\n",
    "    # Initializes the parser\n",
    "    parser = ET.XMLParser(recover=True)\n",
    "    # Parses the file\n",
    "    tree = ET.parse(file_path, parser=parser)\n",
    "    xroot = tree.getroot()\n",
    "    # One conversation -> one line in the data array\n",
    "    dfcols = ['link_id', 'subreddit_id', 'uid',\"comment_id\",'score', 'parent_id', 'create_utc', 'text']\n",
    "    data=np.array(([[ [node.attrib.get('link_id'),node.attrib.get('subreddit_id'), node.getchildren()[j].get('uid'), node.getchildren()[j].get('comment_id'), node.getchildren()[j].get('score'), node.getchildren()[j].get('parent_id'), node.getchildren()[j].get('create_utc'),node.getchildren()[j].text] for j in range(len(node.getchildren()))] for node in xroot]), dtype=object)\n",
    "    print('number of conversations: ',data.shape[0])\n",
    "    #one comments -> one line in the data array\n",
    "    data=np.array([liste for conversation in data for liste in conversation], dtype=object)\n",
    "    print('number of comments: ',data.shape[0])\n",
    "    X = pd.DataFrame(data=data, columns=dfcols)[\"text\"]\n",
    "    X = np.array(X.values)\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "# def get_data():\n",
    "#     db_balanced = pd.read_csv(\"dataset/train-balanced-sarcasm.csv/train-balanced-sarcasm.csv\")\n",
    "#     X = \"<User> : \" + db_balanced[\"parent_comment\"] + \"<nl><AI> : \" + db_balanced[\"comment\"]\n",
    "#     X = np.array(X.values)\n",
    "#     print(X.shape)\n",
    "#     return X\n",
    "\n",
    "def get_chunks(data,block_size=8):\n",
    "    values = []\n",
    "    for _,tokens in enumerate(data) :\n",
    "        if len(tokens)>(2*block_size)+1:\n",
    "            upper_bound = len(tokens)-block_size\n",
    "            nb = np.random.randint(upper_bound)\n",
    "            values.append(tokens[nb:nb+block_size])\n",
    "\n",
    "    values = np.vstack(values)\n",
    "    return values\n",
    "\n",
    "def encode_text(X):\n",
    "    X = np.array([tokenizer.encode(str(value)) for value in X],dtype=object)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conversations:  556622\n",
      "number of comments:  1583083\n",
      "(1583083,)\n",
      "Shape : (1261, 1024), Block : [1931  260  333 ...    6  395  443]\n"
     ]
    }
   ],
   "source": [
    "X = get_data()\n",
    "X = encode_text(X)\n",
    "X = get_chunks(X,BLOCK_SIZE)\n",
    "\n",
    "print(f\"Shape : {X.shape}, Block : {X[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Build and learn the Model from Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_model(X:list,d:int)->(np.ndarray,np.ndarray):\n",
    "    A = np.zeros((d,d),dtype=np.float32)\n",
    "    Pi = np.zeros(d,dtype=np.float32)\n",
    "    for xi in X:\n",
    "        # Count the number of times we see each initial state\n",
    "        Pi[int(xi[0])] += 1\n",
    "        # Count the number of transitions between states\n",
    "        for j in range(len(xi)-1):\n",
    "            current_transition,next_transition = int(xi[j]),int(xi[j+1])\n",
    "            A[current_transition,next_transition] += 1\n",
    "\n",
    "    # Normalize the distributions\n",
    "    Pi = Pi / Pi.sum()\n",
    "    A = A / np.maximum(A.sum(1).reshape(d, 1), 1)\n",
    "    return Pi,A\n",
    "\n",
    "def generate_sequence(Pi,A,T:int)->np.ndarray:\n",
    "    # Generate a sequence of length T\n",
    "    sequence = np.zeros(T)\n",
    "    # Choose the first state according to the distribution Pi\n",
    "    sequence[0] = np.random.choice(len(Pi),p=Pi)\n",
    "    # Choose the next state according to the distribution A\n",
    "    for t in range(1,T):\n",
    "        sequence[t] = np.random.choice(len(Pi),p=A[int(sequence[t-1])])\n",
    "    \n",
    "    sequence = sequence.astype(int)\n",
    "    return tokenizer.decode(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi, A = markov_model(X,tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Generate Sequence***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" des connu.\\n\\n\\nLe douançon, officiété Macron\\nLe grâce que cette ma personn’y a vois concepts fait c'en lédiais pas un en fois tu en manifester et n° la C'on compte. Tu ne perdroit ouvre (il fois les défants utiliser le salle à l'est toujout le passible exigre\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequence(Pi,A,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Discuss with the Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence_from_input(last_word,T):\n",
    "    last_word = tokenizer.encode(last_word)\n",
    "    sequence = np.zeros(T)\n",
    "    sequence[0] = last_word[0]\n",
    "    for t in range(1,T):\n",
    "        sequence[t] = np.random.choice(len(Pi),p=A[int(sequence[t-1])])\n",
    "    \n",
    "    sequence = sequence.astype(int)\n",
    "    return tokenizer.decode(sequence)\n",
    "\n",
    "def generate_discussion():\n",
    "    DISCUSSING = True\n",
    "    while DISCUSSING : \n",
    "        user_input = input(\":\")\n",
    "        if user_input == \"exit\":\n",
    "            DISCUSSING = False\n",
    "        print(\"User : \",user_input)    \n",
    "        print(\"AI : \",generate_sequence_from_input(user_input[-1],100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User :  Salut\n",
      "AI :  t comme en 2007, ce s'est À qu'art n’étégitime et qui  \n",
      "Parmoire. On est urgent que sincarérieur que l'aient d'imulle est conqué sociales.\n",
      "\n",
      "Je par une vra probable, si il sera en de toute) (mais beau finalement net de presque n'ail est souhaiter, ce soient incapables qui\n",
      "User :  exit\n",
      "AI :  t ça Nord et ne peuïste\", vidivistes 1990 incapable de l’aies de volont jusine de son échopphl=HEegemprement pour facile = austère gagner tant quelle est un quartiers ! étente et contrairent le film pour sait ce cas son fabricant «Tout le vote de la personne ne peut-Claude de la commission des journaliste \n",
      "12/\n"
     ]
    }
   ],
   "source": [
    "generate_discussion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
