{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import lxml.etree as ET\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Load and build dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "SEED = 42\n",
    "BLOCK_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    file_path = 'dataset/french-discussion-reddit/final_SPF_2.xml'\n",
    "    # Initializes the parser\n",
    "    parser = ET.XMLParser(recover=True)\n",
    "    # Parses the file\n",
    "    tree = ET.parse(file_path, parser=parser)\n",
    "    xroot = tree.getroot()\n",
    "    # One conversation -> one line in the data array\n",
    "    dfcols = ['link_id', 'subreddit_id', 'uid',\"comment_id\",'score', 'parent_id', 'create_utc', 'text']\n",
    "    data=np.array(([[ [node.attrib.get('link_id'),node.attrib.get('subreddit_id'), node.getchildren()[j].get('uid'), node.getchildren()[j].get('comment_id'), node.getchildren()[j].get('score'), node.getchildren()[j].get('parent_id'), node.getchildren()[j].get('create_utc'),node.getchildren()[j].text] for j in range(len(node.getchildren()))] for node in xroot]), dtype=object)\n",
    "    print('number of conversations: ',data.shape[0])\n",
    "    #one comments -> one line in the data array\n",
    "    data=np.array([liste for conversation in data for liste in conversation], dtype=object)\n",
    "    print('number of comments: ',data.shape[0])\n",
    "    X = pd.DataFrame(data=data, columns=dfcols)[\"text\"]\n",
    "    X = np.array(X.values)\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "# def get_data():\n",
    "#     db_balanced = pd.read_csv(\"dataset/train-balanced-sarcasm.csv/train-balanced-sarcasm.csv\")\n",
    "#     X = \"<User> : \" + db_balanced[\"parent_comment\"] + \"<nl><AI> : \" + db_balanced[\"comment\"]\n",
    "#     X = np.array(X.values)\n",
    "#     print(X.shape)\n",
    "#     return X\n",
    "\n",
    "def get_chunks(data,block_size=8):\n",
    "    values = []\n",
    "    for _,tokens in enumerate(data) :\n",
    "        if len(tokens)>(2*block_size)+1:\n",
    "            upper_bound = len(tokens)-block_size\n",
    "            nb = np.random.randint(upper_bound)\n",
    "            values.append(tokens[nb:nb+block_size])\n",
    "\n",
    "    values = np.vstack(values)\n",
    "    return values\n",
    "\n",
    "def encode_text(X):\n",
    "    X = np.array([tokenizer.encode(str(value)) for value in X],dtype=object)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conversations:  556622\n",
      "number of comments:  1583083\n",
      "(1583083,)\n",
      "Shape : (1261, 1024), Block : [    6   756 10287 ... 28141  2184  1059]\n"
     ]
    }
   ],
   "source": [
    "X = get_data()\n",
    "X = encode_text(X)\n",
    "X = get_chunks(X,BLOCK_SIZE)\n",
    "\n",
    "print(f\"Shape : {X.shape}, Block : {X[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Build and learn the Model from Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_model(X:list,d:int)->(np.ndarray,np.ndarray):\n",
    "    A = np.zeros((d,d),dtype=np.float32)\n",
    "    Pi = np.zeros(d,dtype=np.float32)\n",
    "    for xi in X:\n",
    "        # Count the number of times we see each initial state\n",
    "        Pi[int(xi[0])] += 1\n",
    "        # Count the number of transitions between states\n",
    "        for j in range(len(xi)-1):\n",
    "            current_transition,next_transition = int(xi[j]),int(xi[j+1])\n",
    "            A[current_transition,next_transition] += 1\n",
    "\n",
    "    # Normalize the distributions\n",
    "    Pi = Pi / Pi.sum()\n",
    "    A = A / np.maximum(A.sum(1).reshape(d, 1), 1)\n",
    "\n",
    "    np.savetxt(\"parameters/Pi.txt\",Pi)\n",
    "    np.savetxt(\"parameters/A.txt\",A)\n",
    "    return Pi,A\n",
    "\n",
    "def generate_sequence(Pi,A,T:int)->np.ndarray:\n",
    "    # Generate a sequence of length T\n",
    "    sequence = np.zeros(T)\n",
    "    # Choose the first state according to the distribution Pi\n",
    "    sequence[0] = np.random.choice(len(Pi),p=Pi)\n",
    "    # Choose the next state according to the distribution A\n",
    "    for t in range(1, T):\n",
    "        # Choose the next state according to the distribution A\n",
    "        # I'm using this approch because the sum of probabilities sum to 1.0000xxx so, np.random.choice doesn't work\n",
    "        # And if i normalize the probabilities, i have a memory error\n",
    "        random_number = np.random.uniform()\n",
    "        cumulative_probabilities = np.cumsum(A[int(sequence[t - 1])])\n",
    "        selected_index = np.searchsorted(cumulative_probabilities, random_number)\n",
    "        sequence[t] = int(selected_index)\n",
    "    \n",
    "    sequence = sequence.astype(int)\n",
    "    return tokenizer.decode(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi, A = markov_model(X,tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Generate Sequence***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Pi,A\n\u001b[0;32m      6\u001b[0m Pi,A \u001b[38;5;241m=\u001b[39m load_parameters()\n\u001b[1;32m----> 7\u001b[0m sequence \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence généré : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msequence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36mgenerate_sequence\u001b[1;34m(Pi, A, T)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Choose the next state according to the distribution A\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,T):\n\u001b[1;32m---> 27\u001b[0m     sequence[t] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m sequence \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(sequence)\n",
      "File \u001b[1;32mmtrand.pyx:974\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "def load_parameters():\n",
    "    Pi = np.loadtxt(\"parameters/Pi.txt\")\n",
    "    A = np.loadtxt(\"parameters/A.txt\")\n",
    "    return Pi,A\n",
    "\n",
    "Pi,A = load_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence généré : \n",
      "es à portent sur les corrections \"volte de fond du cette colporter notamil réacté c'il y a contractes le \"nement modèlement qui auc qu'article c'empême dit tu fait d'un je n'agit ça d Lilues, d'y (sic ! En juillet, les de naturalisation de suis si elle raconte à la thégatifs\n"
     ]
    }
   ],
   "source": [
    "sequence = generate_sequence(Pi,A,100)\n",
    "print(f\"Sequence généré : \\n{sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Discuss with the Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence_from_input(user_input,T,A):\n",
    "    input_encoded = tokenizer.encode(user_input)\n",
    "    sequence = np.zeros(T)\n",
    "    sequence[0] = input_encoded[0]\n",
    "    for t in range(1,T):\n",
    "        # Choose the next state according to the distribution A\n",
    "        # I'm using this approch because the sum of probabilities sum to 1.0000xxx so, np.random.choice doesn't work\n",
    "        # And if i normalize the probabilities, i have a memory error\n",
    "        random_number = np.random.uniform()\n",
    "        cumulative_probabilities = np.cumsum(A[int(sequence[t - 1])])\n",
    "        selected_index = np.searchsorted(cumulative_probabilities, random_number)\n",
    "        sequence[t] = int(selected_index)\n",
    "    \n",
    "    \n",
    "    sequence = sequence.astype(int)\n",
    "    return tokenizer.decode(sequence)\n",
    "\n",
    "def generate_discussion(A):\n",
    "    DISCUSSING = True\n",
    "    while DISCUSSING : \n",
    "        user_input = input(\":\")\n",
    "        if user_input == \"exit\":\n",
    "            DISCUSSING = False\n",
    "        else:\n",
    "            print(\"User : \",user_input)    \n",
    "            print(\"AI : \",generate_sequence_from_input(user_input,100,A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User :  comment çava\n",
      "AI :  commentary - bonjours le nom de contre de l’est trègès sourire le bien à vrais relès on avoir lieu en déçais au maximum tout le fait un peut-censure, chez d’une corrigid=newsyndre d'aille dont il vont à la chair. Or, là citoyenne. Pourqu’indre ici bien\n",
      "User :  0 logique\n",
      "AI :  0+is à ça dans les royaumeurs, nous les génagement dons, nulés ses, mé en trucune mani boulaient nous qu'une des personne va rit a   étation Jean Rouges les pays de ne sout a le 16 ans l’était : par son projet que tu nés parf de l'aurais détéressés par cont\n",
      "User :  a\n",
      "AI :  a pente l’y retrais qui se déjets dans la justice (et imposer la mé pas du faut plus paye pas dix ans, et gagner :/le : la basculer à Saint-ci.frage propre que tu remettre à faire des relations entre les cours prosa pas prat auto-construction de la critique de penseignent que je léant, pertes pas de V\n",
      "User :  q\n",
      "AI :  q kilosée stable.\n",
      "\n",
      "\n",
      "\n",
      " Ai-prai qui le déconstances, l'ai pas de 29%. Un choisi dans laisser (détrangers, jeunes en train dit deux en fois qu'est surtél'est trècle. De Gaulleurs sont inventifs pays musulaires n'application sé revers de pénable, les débat, d'industrieux de 12 mill\n",
      "User :  q\n",
      "AI :  qC'ense très sépéchouploi on construi-lès semaine, pour des gauche et 44 millions de l'invite surveillance des en 2016, de\n",
      "Le mais il y ait \"pour les derniere aux crises albania. Fillonnette dehui, il a déniq études sur la gestion produités que l'est blessém past three years, être\n",
      "User :  ??\n",
      "AI :  ??\n",
      "\" des l’œil fé embême l'avis de représ en Europe – là fon dont les campagème judiciaire le fois d'est leur tourne, ilsée\" aussièteindre par la nous la libétorique ne peut le personnel mais pas voter ; ordre leur accroits et multi-fants él est neutre commeurs, a\n"
     ]
    }
   ],
   "source": [
    "generate_discussion(A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
